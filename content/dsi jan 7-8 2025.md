predictor variable: dimension used to predict a response
response variable: responds to changes

statistical learning estimates relationship between predictors and response variables

**prediction vs inference**
Prediction: can we predict future sales
make accurate guesses as to what will happen

Inference: can we understand how changes in budget affect sales

how to estimate our outcome? training vs testing data

supervised: predict something based on other information

unsupervised: understand relationships between diff pieces of information without predicting anything


**regression vs classification**

regression: predict a number
classification: predict categories, like whether someone will buy a product (yes/no)

regression seems to allow continuous values, classification is discrete

**classifier** assigns class to new observations based on their similarity to known observations (training set). already labelled

KNN - finds nearest or most similar observations. K is a predefined number of neighbors considered for classification![[Pasted image 20250107182739.png]]

KNN - majority vote based on nearest/closest neighbours
- Determine the nearest obs to the new obs from the training set
- Identify the K observations that are closest
- Classify the new observation based on the most common class among these neighbors

issue with low K is that it is sensitive to noise.

![[Pasted image 20250107183913.png]]

standardize variable by subtracting the mean and dividing the std for each value to account for variable scaling having an effect on the prediction process.

such that mean 0 and std of 1, allowing the model to consider each variable equally based on its relationship to the outcome rather than its scale

another issue: if one class has more data points than others, KNN more likely to predict those. dataset should contain an event amount of each class

ignoring missing data issues can result in misleading analyses and skew results, introduce bias

handling missing data: remove observations with missing entries. impute/fill in missing data with estimated (mean) data. affects variability

---
folds
k


25% of data
	Used for testing, evaluating model performance using accuracy score
75% of data
	N folds
		1 fold for validation
		N-1 folds for training

execution of knn:

1. split into training and test data.
2. initialize KNeighborsClassifier model
3. Define parameter grid: range of k values to tune
4. Perform grid search: GridSearchCV with parameter grid to estimate accuracy for different k values. 
5. Execute Grid Search: fit the GridSearch
6. select the best k
7. retrain model on full training data
8. evaluate model on test set using score method

manual knn: calculate straight line distance from new obs to every obs in training data. find k smallest neighbors. majority vote. random tie breaker (for even k)


```python
cancer["dist_from_new"] = (
      (cancer["perimeter_mean"] - new_obs_Perimeter) ** 2
    + (cancer["concavity_mean"] - new_obs_Concavity) ** 2
    + (cancer["symmetry_mean"] - new_obs_Symmetry) ** 2
)**(1/2)
```

methods used

pandas.read_csv()
df.unique()
df.value_counts()
df.info()
df.head()
df.replace()
df.groupby()
df.size()
KNeighborsClassifier
knn.score()
df.crosstab( diagnosis, predicted)

<table>
  <tbody><tr>
    <th></th>
    <th>Predicted Malignant</th>
    <th>Predicted Benign</th>
  </tr>
  <tr>
    <th>Actually Malignant</th>
    <td>True Positive</td>
    <td>False Negative</td>
  </tr>
  <tr>
    <th>Actually Benign</th>
    <td>False Positive</td>
    <td>True Negative</td>
  </tr>
</tbody></table>

